{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOG3pXs8TxLY"
      },
      "source": [
        "# Lab 4: TableGPT\n",
        "\n",
        "In this lab, we'll discover the power of code generation models through TableGPT2. The aim is to see how the model can be used in data analysis.\n",
        "\n",
        "First of all, the notebook is divided into X sections: 0. Installation: This section is dedicated to module installation, model loading and data loading.\n",
        "\n",
        "1. Guided introduction: Together, we'll discover how to use and evaluate TableGPT2.\n",
        "2. More questions: You'll need to add at least one new question type to our simple evaluation system.\n",
        "3. More data sets: You'll need to implement a question with multiple datasets.\n",
        "\n",
        "IMPORTANT:\n",
        "\n",
        "- You must work in pairs. You must submit **ONLY ONE NOTEBOOK** for each pair.\n",
        "- Do not share your work with other pairs.\n",
        "- You should not use Copilot, ChatGPT or similar tools. At the very least, remove the prompt ...\n",
        "- <font color='red'>All the things you need to do are indicated in red.</font>\n",
        "\n",
        "<font color='red'>**FIRST QUESTION:** What are the specificty of the TableGPT2 model?</font> https://huggingface.co/tablegpt/TableGPT2-7B\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "TableGPT2-7B is a large-scale decoder designed for data-intensive tasks, particularly those involving tabular data, such as business intelligence (BI), automated analysis, and database applications.\n",
        "\n",
        "It is based on the Qwen2.5 architecture and features specialized encoding to process structured data from tables. The model supports Chinese as its primary language and offers strong performance in coding, data interpretation, and BI-focused query answering.\n",
        "\n",
        "With 7 billion parameters, it has been trained on multimodal BI data, including 86 billion tokens and nearly 600,000 tables. TableGPT2-7B is open-sourced as a standalone decoder, with plans to release a specialized encoder for tighter integration with DeepSpeed and vLLM in the future."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az4ycBgrXuDU"
      },
      "source": [
        "## 0. Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MlTjE4nDH05G"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers datasets bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TYDBffMEX5Pw"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    GenerationConfig,\n",
        ")\n",
        "\n",
        "import pandas as pd\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DU7OdU7mzyfN"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a2d0e0ac85b4d2b9f434eefb7a09591",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Qwen2ForCausalLM(\n",
              "  (model): Qwen2Model(\n",
              "    (embed_tokens): Embedding(152064, 3584)\n",
              "    (layers): ModuleList(\n",
              "      (0-27): 28 x Qwen2DecoderLayer(\n",
              "        (self_attn): Qwen2Attention(\n",
              "          (q_proj): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
              "          (k_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "          (v_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "          (o_proj): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
              "        )\n",
              "        (mlp): Qwen2MLP(\n",
              "          (gate_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "    (rotary_emb): Qwen2RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm_name = \"tablegpt/TableGPT2-7B\"\n",
        "\n",
        "# We want to use 4bit quantization to save memory\n",
        "quantization_config = BitsAndBytesConfig(load_in_8bit=False, load_in_4bit=True)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(llm_name, padding_side=\"left\")\n",
        "# Prevent some transformers specific issues.\n",
        "tokenizer.use_default_system_prompt = False\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Load LLM.\n",
        "llm = AutoModelForCausalLM.from_pretrained(\n",
        "    llm_name,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"cuda\",  # load all the model layers on GPU 0\n",
        "    torch_dtype=torch.bfloat16,  # float precision\n",
        ")\n",
        "# Set LLM on eval mode.\n",
        "llm.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HAciK5cIWxgb"
      },
      "outputs": [],
      "source": [
        "generation_config = GenerationConfig(\n",
        "    max_new_tokens=512,\n",
        "    do_sample=False,\n",
        "    # do_sample=True,\n",
        "    # temperature=.7,\n",
        "    # top_p=.8,\n",
        "    # top_k=20,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ME-E7zK-zZwU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 712 entries, 0 to 890\n",
            "Data columns (total 11 columns):\n",
            " #   Column       Non-Null Count  Dtype  \n",
            "---  ------       --------------  -----  \n",
            " 0   PassengerId  712 non-null    int64  \n",
            " 1   Survived     712 non-null    int64  \n",
            " 2   Pclass       712 non-null    int64  \n",
            " 3   Name         712 non-null    object \n",
            " 4   Sex          712 non-null    object \n",
            " 5   Age          712 non-null    float64\n",
            " 6   SibSp        712 non-null    int64  \n",
            " 7   Parch        712 non-null    int64  \n",
            " 8   Ticket       712 non-null    object \n",
            " 9   Fare         712 non-null    float64\n",
            " 10  Embarked     712 non-null    object \n",
            "dtypes: float64(2), int64(5), object(4)\n",
            "memory usage: 66.8+ KB\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"hf://datasets/phihung/titanic/train.csv\")\n",
        "df = df.drop(\"Cabin\", axis=1).dropna()\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPxFK_SbYIGf"
      },
      "source": [
        "## 1.1 Guided Introduction: The Model.\n",
        "\n",
        "Below there is an example of a prompt that could be used with TableGPT2.\n",
        "\n",
        "```\n",
        "Given access to several pandas dataframes, write the Python code to answer the user's question.\n",
        "The answer should be store in a variable named \"output\".\n",
        "\n",
        "/*\n",
        "\"df.head(5).to_string(index=False)\" as follows:\n",
        " PassengerId  Survived  Pclass                                                Name    Sex  Age  SibSp  Parch           Ticket    Fare Embarked\n",
        "           1         0       3                             Braund, Mr. Owen Harris   male 22.0      1      0        A/5 21171  7.2500        S\n",
        "           2         1       1 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female 38.0      1      0         PC 17599 71.2833        C\n",
        "           3         1       3                              Heikkinen, Miss. Laina female 26.0      0      0 STON/O2. 3101282  7.9250        S\n",
        "           4         1       1        Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0      1      0           113803 53.1000        S\n",
        "           5         0       3                            Allen, Mr. William Henry   male 35.0      0      0           373450  8.0500        S\n",
        "*/\n",
        "\n",
        "Question: How many child survive? (under 18)\n",
        "```\n",
        "\n",
        "The prompt is divided in 3 parts:\n",
        "\n",
        "1. The global instruction wich is to write python that could answer a question on a specific dataset.\n",
        "2. The header of the given dataset: 5 first lines of titanic dataset.\n",
        "3. The question to answer: \"How many child survive? (under 18)\n",
        "\n",
        "First, we will implement a function that generate an answer for this prompt.\n",
        "\n",
        "<font color='red'>TODO: Fill in the `generate_answer` function following the comments inside.</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pvUClWNx1jEj"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Given access to several pandas dataframes, write the Python code to answer the user's question.\n",
            "The answer should be store in a variable named \"output\". Make SURE the answer isn't be a dataframe but a single value (int, float, str, etc.).\n",
            "\n",
            "/*\n",
            "\"df.head(5).to_string(index=False)\" as follows:\n",
            " PassengerId  Survived  Pclass                                                Name    Sex  Age  SibSp  Parch           Ticket    Fare Embarked\n",
            "           1         0       3                             Braund, Mr. Owen Harris   male 22.0      1      0        A/5 21171  7.2500        S\n",
            "           2         1       1 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female 38.0      1      0         PC 17599 71.2833        C\n",
            "           3         1       3                              Heikkinen, Miss. Laina female 26.0      0      0 STON/O2. 3101282  7.9250        S\n",
            "           4         1       1        Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0      1      0           113803 53.1000        S\n",
            "           5         0       3                            Allen, Mr. William Henry   male 35.0      0      0           373450  8.0500        S\n",
            "*/\n",
            "\n",
            "Question: How many child survive? (under 18)\n",
            "\n",
            "\n",
            "*****\n",
            "\n",
            "Python code:\n",
            "```python\n",
            "# Filter the dataframe to include only passengers under the age of 18\n",
            "children = df[df['Age'] < 18]\n",
            "\n",
            "# Count the number of children who survived\n",
            "child_survivors = children[children['Survived'] == 1].shape[0]\n",
            "\n",
            "# Store the answer in the variable \"output\"\n",
            "output = child_survivors\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "example_prompt_template = \"\"\"Given access to several pandas dataframes, write the Python code to answer the user's question.\n",
        "The answer should be store in a variable named \"output\". Make SURE the answer isn't be a dataframe but a single value (int, float, str, etc.).\n",
        "\n",
        "/*\n",
        "\"{var_name}.head(5).to_string(index=False)\" as follows:\n",
        "{df_info}\n",
        "*/\n",
        "\n",
        "Question: {user_question}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def generate_answer(prompt, llm=llm, generation_config=generation_config):\n",
        "    # Create turns with the given prompt.\n",
        "    chat = [\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "\n",
        "    # Apply template with the tokenizer. Be careful to return pt tensors on the same device than `llm`.\n",
        "    chat_encoded = tokenizer.apply_chat_template(chat, return_tensors=\"pt\").to(\n",
        "        llm.device\n",
        "    )\n",
        "\n",
        "    # Generate with llm using the given generation config.\n",
        "    llm_outputs_ids = llm.generate(\n",
        "        input_ids=chat_encoded,\n",
        "        generation_config=generation_config,\n",
        "    )[0]\n",
        "\n",
        "    # Decode and select the answer to return.\n",
        "    answer = tokenizer.decode(\n",
        "        llm_outputs_ids[chat_encoded.size(1) :], skip_special_tokens=True\n",
        "    )\n",
        "    return answer\n",
        "\n",
        "\n",
        "prompt = example_prompt_template.format(\n",
        "    var_name=\"df\",\n",
        "    df_info=df.head(5).to_string(index=False),\n",
        "    user_question=\"How many child survive? (under 18)\",\n",
        ")\n",
        "\n",
        "answer = generate_answer(prompt)\n",
        "\n",
        "print(prompt)\n",
        "print(\"\\n*****\\n\")\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Nt90EerdwBN"
      },
      "source": [
        "## 1.2 Guided Introduction: The Answer.\n",
        "\n",
        "As you can see, the model answer with some generated code.\n",
        "\n",
        "````\n",
        "Python code:\n",
        "```python\n",
        "# Filter the dataframe to include only passengers under the age of 18\n",
        "children = df[df['Age'] < 18]\n",
        "\n",
        "# Count the number of children who survived\n",
        "child_survivors = children[children['Survived'] == 1]\n",
        "\n",
        "# Save the answer in the variable output\n",
        "output = len(child_survivors)\n",
        "````\n",
        "\n",
        "So we will need to execute it, but there is some difficulty:\n",
        "\n",
        "1. Sometime, the llm answer with \\`\\`\\`python ... \\`\\`\\`, sometime the llm answer directly with the code. We need to handle both cases.\n",
        "2. We need to recover the variable output from the execution.\n",
        "3. We need to evaluate single value and list of values.\n",
        "\n",
        "First, we will implement a function that generate an answer for this prompt.\n",
        "\n",
        "<font color='red'>TODO: Fill in the `exec_answer` function following the comments inside.</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "z_hwvxSGMSlo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import re, sys, os\n",
        "\n",
        "\n",
        "def exec_answer(answer, gold, context):\n",
        "    # Extract the code from the answer. Be careful, the code is now always in ``` ```.\n",
        "    if \"```\" in answer:\n",
        "        code = re.search(r\"```python\\n(.*?)\\n```\", answer, re.DOTALL)\n",
        "        code = code.group(1)\n",
        "    else:\n",
        "        code = answer\n",
        "\n",
        "    # Execute the code, https://docs.python.org/3/library/functions.html#exec\n",
        "    # if the code work: Return True or False based on output == gold (be careful to handle iterable !)\n",
        "    # if the code don't work return False.\n",
        "\n",
        "    original_stdout = sys.stdout  # suppress output from exec\n",
        "    try:\n",
        "        sys.stdout = open(os.devnull, \"w\")  # suppress output from exec\n",
        "        exec(code, context)\n",
        "        output = next(\n",
        "            reversed(context.values())\n",
        "        )  # get the last assigned value of the context as output\n",
        "\n",
        "        if isinstance(output, pd.DataFrame) or isinstance(output, pd.Series):\n",
        "            raise Exception(\"Output is a DataFrame or Series, please return a scalar.\")\n",
        "\n",
        "        sys.stdout = original_stdout\n",
        "        return output == gold\n",
        "    except Exception as e:\n",
        "        sys.stdout = original_stdout\n",
        "        print(e)\n",
        "        return False\n",
        "\n",
        "\n",
        "print(exec_answer(answer, 61, {\"df\": df}))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1TrmGcQIEeI"
      },
      "source": [
        "## 1.3 Guided Introduction: The Question.\n",
        "\n",
        "Now we want to automatically generate questions to evaluate the performance of our model. There are benchmarks on this subject, but here we want to practice code by generating the questions ourselves.\n",
        "\n",
        "We will generate some basic filter questions.\n",
        "\n",
        "<font color='red'>TODO: Fill in the `generate_filter_question` function following the comments inside.</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:13<00:00,  2.69s/it]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'question': \"What is the number of rows where the value in the column 'SibSp' is greater than 3 and the value in the column 'Pclass' is 3?\",\n",
              "  'answer': 23},\n",
              " {'question': \"How many rows in the dataset have the value 'male' in the 'Sex' column and 'Q' in the 'Embarked' column?\",\n",
              "  'answer': 16},\n",
              " {'question': \"How many rows in the dataset have a 'Pclass' value of '3' and a 'Sex' value of 'male'?\",\n",
              "  'answer': 253},\n",
              " {'question': \"How many rows in the dataset have a 'SibSp' value less than or equal to 5 and a 'Sex' value of 'male'?\",\n",
              "  'answer': 453},\n",
              " {'question': 'How many passengers survived with no siblings or spouses aboard?',\n",
              "  'answer': 0}]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import random, operator\n",
        "from tqdm import tqdm\n",
        "\n",
        "categorical_columns = [\"Sex\", \"Pclass\", \"Embarked\", \"Survived\"]\n",
        "numerical_columns = [\"Age\", \"Fare\", \"SibSp\", \"Parch\"]\n",
        "numerical_ops = [\"<\", \">\", \"==\", \"!=\", \"<=\", \">=\"]\n",
        "operators = {\n",
        "    \"<\": operator.lt,\n",
        "    \">\": operator.gt,\n",
        "    \"==\": operator.eq,\n",
        "    \"!=\": operator.ne,\n",
        "    \"<=\": operator.le,\n",
        "    \">=\": operator.ge,\n",
        "}\n",
        "\n",
        "\n",
        "def generate_random_question(\n",
        "    generate_function, df, categorical_columns, numerical_columns, k, seed=42\n",
        "):\n",
        "    random.seed(seed)\n",
        "    questions = []\n",
        "    for _ in tqdm(range(k)):\n",
        "        question = generate_function(df, categorical_columns, numerical_columns)\n",
        "        questions.append(question)\n",
        "    return questions\n",
        "\n",
        "\n",
        "def generate_filter_question(df, categorical_columns, numerical_columns):\n",
        "    # Get a random target column and a random filter column (be careful they should be different)\n",
        "    # Get a random filter value inside the filter column. Avoid NaN values.\n",
        "    filter_col = random.choice(categorical_columns + numerical_columns)\n",
        "    if filter_col in categorical_columns:  # Categorical column\n",
        "        filter_val = random.choice(df[filter_col].dropna().unique())\n",
        "    else:  # Numerical column\n",
        "        filter_val = random.choice(df[filter_col].dropna().unique())\n",
        "\n",
        "    target_col = random.choice(\n",
        "        [col for col in [*categorical_columns, *numerical_columns] if col != filter_col]\n",
        "    )\n",
        "    if target_col in categorical_columns:  # Categorical column\n",
        "        target_val = random.choice(df[target_col].dropna().unique())\n",
        "    else:  # Numerical column\n",
        "        target_val = random.choice(df[target_col].dropna().unique())\n",
        "\n",
        "    filter_op = random.choice(numerical_ops)\n",
        "    target_op = random.choice(numerical_ops)\n",
        "\n",
        "    # Create a question template that take a target column, a filter column and a filter value\n",
        "    # WE ARE ALSO ADDING A TARGET VALUE, OTHERWISE THE ANSWER IS IMPOSSIBLE TO COMPUTE\n",
        "    template = (\n",
        "        \"Generate a question based on a tabular dataset with this format:\\n\"\n",
        "        \"{df_info}\\n\"\n",
        "        \"The question should be about the number of rows where the value in the column '{target_col}' is {target_cond} \"\n",
        "        \"and the value in the column '{filter_col}' is {filter_cond}.\"\n",
        "    )\n",
        "\n",
        "    # Determine the conditions for target_col and filter_col\n",
        "    target_cond = (\n",
        "        f\"'{target_val}'\"\n",
        "        if target_col in categorical_columns\n",
        "        else f\"{target_op} '{target_val}'\"\n",
        "    )\n",
        "    filter_cond = (\n",
        "        f\"'{filter_val}'\"\n",
        "        if filter_col in categorical_columns\n",
        "        else f\"{filter_op} '{filter_val}'\"\n",
        "    )\n",
        "\n",
        "    content = template.format(\n",
        "        df_info=df.head(5).to_string(index=False),\n",
        "        target_col=target_col,\n",
        "        target_cond=target_cond,\n",
        "        filter_col=filter_col,\n",
        "        filter_cond=filter_cond,\n",
        "    )\n",
        "\n",
        "    chat = [\n",
        "        {\"role\": \"user\", \"content\": content},\n",
        "    ]\n",
        "    chat_encoded = tokenizer.apply_chat_template(chat, return_tensors=\"pt\").to(\n",
        "        llm.device\n",
        "    )\n",
        "\n",
        "    llm_outputs_ids = llm.generate(\n",
        "        input_ids=chat_encoded,\n",
        "        generation_config=generation_config,\n",
        "    )[0]\n",
        "    question = tokenizer.decode(\n",
        "        llm_outputs_ids[chat_encoded.size(1) :], skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    # Compute the correct answer based on the question\n",
        "    if filter_col in categorical_columns and target_col in categorical_columns:\n",
        "        correct_answer = df[\n",
        "            (df[target_col] == target_val) & (df[filter_col] == filter_val)\n",
        "        ].shape[0]\n",
        "    elif filter_col in numerical_columns and target_col in categorical_columns:\n",
        "        correct_answer = df[\n",
        "            (df[target_col] == target_val)\n",
        "            & operators[filter_op](df[filter_col], filter_val)\n",
        "        ].shape[0]\n",
        "    elif filter_col in categorical_columns and target_col in numerical_columns:\n",
        "        correct_answer = df[\n",
        "            operators[target_op](df[target_col], target_val)\n",
        "            & (df[filter_col] == filter_val)\n",
        "        ].shape[0]\n",
        "    else:\n",
        "        correct_answer = df[\n",
        "            operators[target_op](df[target_col], target_val)\n",
        "            & operators[filter_op](df[filter_col], filter_val)\n",
        "        ].shape[0]\n",
        "\n",
        "    # return formatted question and associated answer in a dict {\"question\":[question], \"answer\":[answer]}\n",
        "    return {\"question\": question, \"answer\": correct_answer}\n",
        "\n",
        "\n",
        "generate_random_question(\n",
        "    generate_filter_question, df, categorical_columns, numerical_columns, 5\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To generate the questions we used a hybrid approach: we generate the question with a llm, but we constraint the type of question generated, by providing an example built with a template.\n",
        "Since we need to calculate the true answer to evaluate the model, we need to know in advance which columns are we using (as filter and target) ans which operation are we performing on them.\n",
        "\n",
        "The main steps to generate a question are summarized below:\n",
        "\n",
        "1. Define the columns and operations: Determine the columns and operations that will be used in the question. This includes identifying the filter column and the target column, as well as the operation to be performed on them.\n",
        "\n",
        "2. Generate the question template: Create a question template that includes placeholders for the filter column, target column, and operation. This template will be used to generate the final question.\n",
        "\n",
        "3. Generate the question prompt: Use the question template as a prompt to generate the question.\n",
        "\n",
        "4. Generate the true answer: Calculate the true answer to the question based on the filter column, target column, and operation. This will be used to evaluate the model's performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzDJ9c3VhHt2"
      },
      "source": [
        "## 1.4 Guided Introduction: The Evaluation.\n",
        "\n",
        "The last step in this section is to evaluate our model on 20 random questions! We'll use simple accuracy.\n",
        "\n",
        "You should have an accuracy between 0.9 and 1.\n",
        "\n",
        "<font color='red'>TODO: Follow instruction in comment of the cell below.</font>\n",
        "\n",
        "<font color='green'>BONUS: Investigate on errors and improve our prompt/parsing to solve them.</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "xiE53oszRGcq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating random questions\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [01:02<00:00,  3.13s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting the evaluation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [01:44<00:00,  5.21s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 19/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Generate 20 random question\n",
        "print(\"Generating random questions\")\n",
        "questions = generate_random_question(\n",
        "    generate_filter_question, df, categorical_columns, numerical_columns, 20\n",
        ")\n",
        "sum_correct = 0\n",
        "incorrect_answers = []\n",
        "\n",
        "# Iterate over question to format prompt, generate answer and execute answer.\n",
        "print(\"\\nStarting the evaluation\")\n",
        "for q in tqdm(questions):\n",
        "    prompt = example_prompt_template.format(\n",
        "        var_name=\"df\",\n",
        "        df_info=df.head(5).to_string(index=False),\n",
        "        user_question=q[\"question\"],\n",
        "    )\n",
        "    answer = generate_answer(prompt)\n",
        "    res = exec_answer(answer, q[\"answer\"], {\"df\": df})\n",
        "    if res == False:\n",
        "        incorrect_answers.append((q, answer))\n",
        "\n",
        "    sum_correct += res\n",
        "\n",
        "# Report the Accuracy\n",
        "print(f\"Accuracy: {sum_correct}/{len(questions)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looking at the results, we can see that the model is able to generate code that can answer simple questions.\n",
        "\n",
        "We obtained this high accuracy by fixing some of the issues we encountered during the guided introduction.\n",
        "In particular the model tended to:\n",
        "- output a dataframe instead of a number (fixed by specifying in the answer prompt that the output should be a number)\n",
        "- printing, and not assigning the results to the \"output\" variable (fixed by taking as output the last assigned variable)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGYL5LUeiEBI"
      },
      "source": [
        "## 2. More Questions.\n",
        "\n",
        "Now it's your turn to imagine a type of question (\"How many ...\"). Implement a function to generate new type of question. Verify that our previous code work with your new question then evaluate it.\n",
        "\n",
        "<font color='red'>TODO: Generate **AT LEAST ONE** new type of question and report this new question accuracy.</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def generate_fare_question(df, categorical_columns, numerical_columns):\n",
        "    # Get a random target column and a random filter column (be careful they should be different)\n",
        "    # Get a random filter value inside the filter column. Avoid NaN values.\n",
        "    filter_col = random.choice(\n",
        "        [col for col in [*categorical_columns, *numerical_columns] if col != \"Fare\"]\n",
        "    )\n",
        "    filter_val = random.choice(df[filter_col].dropna().unique())\n",
        "    filter_op = random.choice(numerical_ops)\n",
        "\n",
        "    # Create a question template that take a target column, a filter column and a filter value\n",
        "    if filter_col in categorical_columns:\n",
        "        template = (\n",
        "            \"Generate a question based on a tabular dataset with this format:\\n\"\n",
        "            \"{df_info}\\n\"\n",
        "            \"The question should be about the total fare for the passengers where \"\n",
        "            \"the value in the column '{filter_col}' is '{filter_val}'.\"\n",
        "            \"Here are some examples of questions:\"\n",
        "            \"1) What is the total fare for the passengers who survived?\"\n",
        "            \"2) How much did the passengers who embarked in S pay in total?\"\n",
        "            \"3) What is the total fare for male passengers?\"\n",
        "        )\n",
        "    else:\n",
        "        template = (\n",
        "            \"Generate a question based on a tabular dataset with this format:\\n\"\n",
        "            \"{df_info}\\n\"\n",
        "            \"The question should be about the total fare for the passengers where \"\n",
        "            \"the value in the column '{filter_col}' is {filter_op} '{filter_val}'.\"\n",
        "            \"Here are some examples of questions:\"\n",
        "            \"1) What is the total fare for the passengers who are older than 30?\"\n",
        "            \"2) How much did the passengers who paid more than 50 in total?\"\n",
        "            \"3) What is the total fare for passengers who have more than 2 siblings?\"\n",
        "        )\n",
        "\n",
        "    content = template.format(\n",
        "        df_info=df.head(5).to_string(index=False),\n",
        "        filter_col=filter_col,\n",
        "        filter_val=filter_val,\n",
        "        filter_op=filter_op,\n",
        "    )\n",
        "\n",
        "    chat = [\n",
        "        {\"role\": \"user\", \"content\": content},\n",
        "    ]\n",
        "    chat_encoded = tokenizer.apply_chat_template(chat, return_tensors=\"pt\").to(\n",
        "        llm.device\n",
        "    )\n",
        "\n",
        "    llm_outputs_ids = llm.generate(\n",
        "        input_ids=chat_encoded,\n",
        "        generation_config=generation_config,\n",
        "    )[0]\n",
        "    question = tokenizer.decode(\n",
        "        llm_outputs_ids[chat_encoded.size(1) :], skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    # Compute the correct answer for the given target column, filter column and filter value.\n",
        "    if filter_col in categorical_columns:\n",
        "        correct_answer = df[(df[filter_col] == filter_val)][\"Fare\"].sum()\n",
        "    else:\n",
        "        correct_answer = df[operators[filter_op](df[filter_col], filter_val)][\n",
        "            \"Fare\"\n",
        "        ].sum()\n",
        "\n",
        "    # return formatted question and associated answer in a dict {\"question\":[question], \"answer\":[answer]}\n",
        "    return {\"question\": question, \"answer\": correct_answer}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating random questions\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:47<00:00,  2.37s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting the evaluation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [01:33<00:00,  4.68s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 20/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Generate 20 random question\n",
        "print(\"Generating random questions\")\n",
        "questions = generate_random_question(\n",
        "    generate_fare_question, df, categorical_columns, numerical_columns, 20\n",
        ")\n",
        "sum_correct = 0\n",
        "incorrect_answers = []\n",
        "\n",
        "# Iterate over question to format prompt, generate answer and execute answer.\n",
        "print(\"\\nStarting the evaluation\")\n",
        "for q in tqdm(questions):\n",
        "    prompt = example_prompt_template.format(\n",
        "        var_name=\"df\",\n",
        "        df_info=df.head(5).to_string(index=False),\n",
        "        user_question=q[\"question\"],\n",
        "    )\n",
        "    answer = generate_answer(prompt)\n",
        "    res = exec_answer(answer, q[\"answer\"], {\"df\": df})\n",
        "    if res == False:\n",
        "        incorrect_answers.append((prompt, q[\"answer\"], answer))\n",
        "\n",
        "    sum_correct += res\n",
        "\n",
        "# Report the Accuracy\n",
        "print(f\"Accuracy: {sum_correct}/{len(questions)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we encountered another problem. Sometimes the generated questions where not correct.\n",
        "\n",
        "We fixed this by using few-shot prompting to guide the model in generating the right format of question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt35rcuMoAdW"
      },
      "source": [
        "## 3. More datasets.\n",
        "\n",
        "Below we load a new dataset: \"adult_income_dataset\".\n",
        "\n",
        "<font color='red'>TODO: Evaluate our questions on this new dataset. Report the accuracy. Comment Any differences.</font>\n",
        "\n",
        "<font color='green'>BONUS: Try to find a prompt that answer this question: What is the mean salary of titanic surviror based on adult dataset.</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "z33vyj_Sk_WX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 48842 entries, 0 to 48841\n",
            "Data columns (total 15 columns):\n",
            " #   Column           Non-Null Count  Dtype \n",
            "---  ------           --------------  ----- \n",
            " 0   age              48842 non-null  int64 \n",
            " 1   workclass        48842 non-null  object\n",
            " 2   fnlwgt           48842 non-null  int64 \n",
            " 3   education        48842 non-null  object\n",
            " 4   educational-num  48842 non-null  int64 \n",
            " 5   marital-status   48842 non-null  object\n",
            " 6   occupation       48842 non-null  object\n",
            " 7   relationship     48842 non-null  object\n",
            " 8   race             48842 non-null  object\n",
            " 9   gender           48842 non-null  object\n",
            " 10  capital-gain     48842 non-null  int64 \n",
            " 11  capital-loss     48842 non-null  int64 \n",
            " 12  hours-per-week   48842 non-null  int64 \n",
            " 13  native-country   48842 non-null  object\n",
            " 14  income           48842 non-null  object\n",
            "dtypes: int64(6), object(9)\n",
            "memory usage: 5.6+ MB\n"
          ]
        }
      ],
      "source": [
        "adult = pd.read_csv(\"hf://datasets/meghana/adult_income_dataset/adult.csv\")\n",
        "adult.info()\n",
        "\n",
        "titanic = df\n",
        "\n",
        "categorical_columns = [\n",
        "    \"workclass\",\n",
        "    \"education\",\n",
        "    \"marital-status\",\n",
        "    \"occupation\",\n",
        "    \"relationship\",\n",
        "    \"race\",\n",
        "    \"gender\",\n",
        "    \"native-country\",\n",
        "    \"income\",\n",
        "]\n",
        "numerical_columns = [\n",
        "    \"age\",\n",
        "    \"fnlwgt\",\n",
        "    \"educational-num\",\n",
        "    \"capital-gain\",\n",
        "    \"capital-loss\",\n",
        "    \"hours-per-week\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating random questions\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [01:56<00:00,  5.81s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting the evaluation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [02:10<00:00,  6.51s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 19/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Generate 20 random question\n",
        "print(\"Generating random questions\")\n",
        "questions = generate_random_question(\n",
        "    generate_filter_question, adult, categorical_columns, numerical_columns, 20\n",
        ")\n",
        "sum_correct = 0\n",
        "incorrect_answers = []\n",
        "\n",
        "# Iterate over question to format prompt, generate answer and execute answer.\n",
        "print(\"\\nStarting the evaluation\")\n",
        "for q in tqdm(questions):\n",
        "    prompt = example_prompt_template.format(\n",
        "        var_name=\"adult\",\n",
        "        df_info=adult.head(5).to_string(index=False),\n",
        "        user_question=q[\"question\"],\n",
        "    )\n",
        "    answer = generate_answer(prompt)\n",
        "    res = exec_answer(answer, q[\"answer\"], {\"adult\": adult})\n",
        "    if res == False:\n",
        "        incorrect_answers.append((q, answer))\n",
        "\n",
        "    sum_correct += res\n",
        "\n",
        "# Report the Accuracy\n",
        "print(f\"Accuracy: {sum_correct}/{len(questions)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We obtain a high accucary on the new dataset as well.\n",
        "\n",
        "This indicates that our question generation system is robust and can be applied to different datasets."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Regarding the **bonus question**, we found that answering the question \"What is the mean salary of titanic survivors based on adult dataset.\" is impossible.\n",
        "\n",
        "This is because the adult dataset doesn't contain information about the salary of individuals. It just has a feature called \"income\" which indicates whether an individual earns more or less than 50k.\n",
        "\n",
        "If the salary was available, we could have the following strategy:\n",
        "\n",
        "- Join the two datasets on the age and sex columns (we don't have the name of the passengers in the adult dataset, or a common identifier)\n",
        "- Filter the joined dataset to include only the survivors\n",
        "- Calculate the mean salary of the survivors"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
