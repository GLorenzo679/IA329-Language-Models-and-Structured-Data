{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOG3pXs8TxLY"
      },
      "source": [
        "# Lab 4: TableGPT\n",
        "\n",
        "In this lab, we'll discover the power of code generation models through TableGPT2. The aim is to see how the model can be used in data analysis.\n",
        "\n",
        "First of all, the notebook is divided into X sections: 0. Installation: This section is dedicated to module installation, model loading and data loading.\n",
        "\n",
        "1. Guided introduction: Together, we'll discover how to use and evaluate TableGPT2.\n",
        "2. More questions: You'll need to add at least one new question type to our simple evaluation system.\n",
        "3. More data sets: You'll need to implement a question with multiple datasets.\n",
        "\n",
        "IMPORTANT:\n",
        "\n",
        "- You must work in pairs. You must submit **ONLY ONE NOTEBOOK** for each pair.\n",
        "- Do not share your work with other pairs.\n",
        "- You should not use Copilot, ChatGPT or similar tools. At the very least, remove the prompt ...\n",
        "- <font color='red'>All the things you need to do are indicated in red.</font>\n",
        "\n",
        "<font color='red'>**FIRST QUESTION:** What are the specificty of the TableGPT2 model?</font> https://huggingface.co/tablegpt/TableGPT2-7B\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "az4ycBgrXuDU"
      },
      "source": [
        "## 0. Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MlTjE4nDH05G"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers datasets bitsandbytes accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "TYDBffMEX5Pw"
      },
      "outputs": [],
      "source": [
        "from transformers import (\n",
        "    BitsAndBytesConfig,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    GenerationConfig,\n",
        ")\n",
        "\n",
        "import pandas as pd\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DU7OdU7mzyfN"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0d8c8756bd3844aa8f7451620b491ee2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "Qwen2ForCausalLM(\n",
              "  (model): Qwen2Model(\n",
              "    (embed_tokens): Embedding(152064, 3584)\n",
              "    (layers): ModuleList(\n",
              "      (0-27): 28 x Qwen2DecoderLayer(\n",
              "        (self_attn): Qwen2Attention(\n",
              "          (q_proj): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
              "          (k_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "          (v_proj): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "          (o_proj): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
              "        )\n",
              "        (mlp): Qwen2MLP(\n",
              "          (gate_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "        (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "      )\n",
              "    )\n",
              "    (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "    (rotary_emb): Qwen2RotaryEmbedding()\n",
              "  )\n",
              "  (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
              ")"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llm_name = \"tablegpt/TableGPT2-7B\"\n",
        "\n",
        "# We want to use 4bit quantization to save memory\n",
        "quantization_config = BitsAndBytesConfig(load_in_8bit=False, load_in_4bit=True)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(llm_name, padding_side=\"left\")\n",
        "# Prevent some transformers specific issues.\n",
        "tokenizer.use_default_system_prompt = False\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Load LLM.\n",
        "llm = AutoModelForCausalLM.from_pretrained(\n",
        "    llm_name,\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"cuda\",  # load all the model layers on GPU 0\n",
        "    torch_dtype=torch.bfloat16,  # float precision\n",
        ")\n",
        "# Set LLM on eval mode.\n",
        "llm.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "HAciK5cIWxgb"
      },
      "outputs": [],
      "source": [
        "generation_config = GenerationConfig(\n",
        "    max_new_tokens=512,\n",
        "    do_sample=False,\n",
        "    # do_sample=True,\n",
        "    # temperature=.7,\n",
        "    # top_p=.8,\n",
        "    # top_k=20,\n",
        "    eos_token_id=tokenizer.eos_token_id,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ME-E7zK-zZwU"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 712 entries, 0 to 890\n",
            "Data columns (total 11 columns):\n",
            " #   Column       Non-Null Count  Dtype  \n",
            "---  ------       --------------  -----  \n",
            " 0   PassengerId  712 non-null    int64  \n",
            " 1   Survived     712 non-null    int64  \n",
            " 2   Pclass       712 non-null    int64  \n",
            " 3   Name         712 non-null    object \n",
            " 4   Sex          712 non-null    object \n",
            " 5   Age          712 non-null    float64\n",
            " 6   SibSp        712 non-null    int64  \n",
            " 7   Parch        712 non-null    int64  \n",
            " 8   Ticket       712 non-null    object \n",
            " 9   Fare         712 non-null    float64\n",
            " 10  Embarked     712 non-null    object \n",
            "dtypes: float64(2), int64(5), object(4)\n",
            "memory usage: 66.8+ KB\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(\"hf://datasets/phihung/titanic/train.csv\")\n",
        "df = df.drop(\"Cabin\", axis=1).dropna()\n",
        "df.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPxFK_SbYIGf"
      },
      "source": [
        "## 1.1 Guided Introduction: The Model.\n",
        "\n",
        "Below there is an example of a prompt that could be used with TableGPT2.\n",
        "\n",
        "```\n",
        "Given access to several pandas dataframes, write the Python code to answer the user's question.\n",
        "The answer should be store in a variable named \"output\".\n",
        "\n",
        "/*\n",
        "\"df.head(5).to_string(index=False)\" as follows:\n",
        " PassengerId  Survived  Pclass                                                Name    Sex  Age  SibSp  Parch           Ticket    Fare Embarked\n",
        "           1         0       3                             Braund, Mr. Owen Harris   male 22.0      1      0        A/5 21171  7.2500        S\n",
        "           2         1       1 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female 38.0      1      0         PC 17599 71.2833        C\n",
        "           3         1       3                              Heikkinen, Miss. Laina female 26.0      0      0 STON/O2. 3101282  7.9250        S\n",
        "           4         1       1        Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0      1      0           113803 53.1000        S\n",
        "           5         0       3                            Allen, Mr. William Henry   male 35.0      0      0           373450  8.0500        S\n",
        "*/\n",
        "\n",
        "Question: How many child survive? (under 18)\n",
        "```\n",
        "\n",
        "The prompt is divided in 3 parts:\n",
        "\n",
        "1. The global instruction wich is to write python that could answer a question on a specific dataset.\n",
        "2. The header of the given dataset: 5 first lines of titanic dataset.\n",
        "3. The question to answer: \"How many child survive? (under 18)\n",
        "\n",
        "First, we will implement a function that generate an answer for this prompt.\n",
        "\n",
        "<font color='red'>TODO: Fill in the `generate_answer` function following the comments inside.</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "pvUClWNx1jEj"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Given access to several pandas dataframes, write the Python code to answer the user's question.\n",
            "The answer should be store in a variable named \"output\". The answer shouldn't be a dataframe but a single value (int, float, str, etc.).\n",
            "\n",
            "/*\n",
            "\"df.head(5).to_string(index=False)\" as follows:\n",
            " PassengerId  Survived  Pclass                                                Name    Sex  Age  SibSp  Parch           Ticket    Fare Embarked\n",
            "           1         0       3                             Braund, Mr. Owen Harris   male 22.0      1      0        A/5 21171  7.2500        S\n",
            "           2         1       1 Cumings, Mrs. John Bradley (Florence Briggs Thayer) female 38.0      1      0         PC 17599 71.2833        C\n",
            "           3         1       3                              Heikkinen, Miss. Laina female 26.0      0      0 STON/O2. 3101282  7.9250        S\n",
            "           4         1       1        Futrelle, Mrs. Jacques Heath (Lily May Peel) female 35.0      1      0           113803 53.1000        S\n",
            "           5         0       3                            Allen, Mr. William Henry   male 35.0      0      0           373450  8.0500        S\n",
            "*/\n",
            "\n",
            "Question: How many child survive? (under 18)\n",
            "\n",
            "\n",
            "*****\n",
            "\n",
            "Python code:\n",
            "```python\n",
            "# Filter the dataframe to include only passengers under the age of 18\n",
            "children = df[df['Age'] < 18]\n",
            "\n",
            "# Count the number of children who survived\n",
            "child_survivors = children[children['Survived'] == 1].shape[0]\n",
            "\n",
            "# Store the answer in the variable \"output\"\n",
            "output = child_survivors\n",
            "```\n"
          ]
        }
      ],
      "source": [
        "example_prompt_template = \"\"\"Given access to several pandas dataframes, write the Python code to answer the user's question.\n",
        "The answer should be store in a variable named \"output\". The answer shouldn't be a dataframe but a single value (int, float, str, etc.).\n",
        "\n",
        "/*\n",
        "\"{var_name}.head(5).to_string(index=False)\" as follows:\n",
        "{df_info}\n",
        "*/\n",
        "\n",
        "Question: {user_question}\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "def generate_answer(prompt, llm=llm, generation_config=generation_config):\n",
        "    # Create turns with the given prompt.\n",
        "    chat = [\n",
        "        {\"role\": \"user\", \"content\": prompt},\n",
        "    ]\n",
        "\n",
        "    # Apply template with the tokenizer. Be careful to return pt tensors on the same device than `llm`.\n",
        "    chat_encoded = tokenizer.apply_chat_template(chat, return_tensors=\"pt\").to(\n",
        "        llm.device\n",
        "    )\n",
        "\n",
        "    # Generate with llm using the given generation config.\n",
        "    llm_outputs_ids = llm.generate(\n",
        "        input_ids=chat_encoded,\n",
        "        generation_config=generation_config,\n",
        "    )[0]\n",
        "\n",
        "    # Decode and select the answer to return.\n",
        "    answer = tokenizer.decode(\n",
        "        llm_outputs_ids[chat_encoded.size(1) :], skip_special_tokens=True\n",
        "    )\n",
        "    return answer\n",
        "\n",
        "\n",
        "prompt = example_prompt_template.format(\n",
        "    var_name=\"df\",\n",
        "    df_info=df.head(5).to_string(index=False),\n",
        "    user_question=\"How many child survive? (under 18)\",\n",
        ")\n",
        "\n",
        "answer = generate_answer(prompt)\n",
        "\n",
        "print(prompt)\n",
        "print(\"\\n*****\\n\")\n",
        "print(answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Nt90EerdwBN"
      },
      "source": [
        "## 1.2 Guided Introduction: The Answer.\n",
        "\n",
        "As you can see, the model answer with some generated code.\n",
        "\n",
        "````\n",
        "Python code:\n",
        "```python\n",
        "# Filter the dataframe to include only passengers under the age of 18\n",
        "children = df[df['Age'] < 18]\n",
        "\n",
        "# Count the number of children who survived\n",
        "child_survivors = children[children['Survived'] == 1]\n",
        "\n",
        "# Save the answer in the variable output\n",
        "output = len(child_survivors)\n",
        "````\n",
        "\n",
        "So we will need to execute it, but there is some difficulty:\n",
        "\n",
        "1. Sometime, the llm answer with \\`\\`\\`python ... \\`\\`\\`, sometime the llm answer directly with the code. We need to handle both cases.\n",
        "2. We need to recover the variable output from the execution.\n",
        "3. We need to evaluate single value and list of values.\n",
        "\n",
        "First, we will implement a function that generate an answer for this prompt.\n",
        "\n",
        "<font color='red'>TODO: Fill in the `exec_answer` function following the comments inside.</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "z_hwvxSGMSlo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "True\n"
          ]
        }
      ],
      "source": [
        "import re, sys, os\n",
        "\n",
        "\n",
        "def exec_answer(answer, gold):\n",
        "    # Extract the code from the answer. Be careful, the code is now always in ``` ```.\n",
        "    if \"```\" in answer:\n",
        "        code = re.search(r\"```python\\n(.*?)\\n```\", answer, re.DOTALL)\n",
        "        code = code.group(1)\n",
        "    else:\n",
        "        code = answer\n",
        "\n",
        "    # Execute the code, https://docs.python.org/3/library/functions.html#exec\n",
        "    # if the code work: Return True or False based on output == gold (be careful to handle iterable !)\n",
        "    # if the code don't work return False.\n",
        "\n",
        "    original_stdout = sys.stdout  # suppress output from exec\n",
        "    try:\n",
        "        sys.stdout = open(os.devnull, \"w\")\n",
        "        context = {\"df\": df}\n",
        "        exec(code, context)\n",
        "        output = next(reversed(context.values()))\n",
        "\n",
        "        if isinstance(output, pd.DataFrame) or isinstance(output, pd.Series):\n",
        "            raise Exception(\"Output is a DataFrame or Series, please return a scalar.\")\n",
        "\n",
        "        sys.stdout = original_stdout\n",
        "        return output == gold\n",
        "    except Exception as e:\n",
        "        sys.stdout = original_stdout\n",
        "        print(e)\n",
        "        return False\n",
        "\n",
        "\n",
        "print(exec_answer(answer, 61))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1TrmGcQIEeI"
      },
      "source": [
        "## 1.3 Guided Introduction: The Question.\n",
        "\n",
        "Now we want to automatically generate questions to evaluate the performance of our model. There are benchmarks on this subject, but here we want to practice code by generating the questions ourselves.\n",
        "\n",
        "We will generate some basic filter questions.\n",
        "\n",
        "<font color='red'>TODO: Fill in the `generate_filter_question` function following the comments inside.</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/5 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 5/5 [00:21<00:00,  4.30s/it]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'question': 'What is the number of passengers who survived and were older than 24.5 years?',\n",
              "  'answer': 170},\n",
              " {'question': \"What is the number of rows where the value in the column 'Age' is greater than or equal to '0.83' and the value in the column 'Survived' is '0'?\",\n",
              "  'answer': 424},\n",
              " {'question': \"To generate the question, consider the following steps:\\n1. Identify the columns 'Sex' and 'Fare' in the dataset.\\n2. Filter the rows where 'Sex' is 'male' and 'Fare' is '10.5167'.\\n3. Count the number of rows that meet these conditions.\\n4. Formulate a question that asks for the count of such rows.\\n\\nWhat is the number of rows where the value in the column 'Sex' is 'male' and the value in the column 'Fare' is '10.5167'?\",\n",
              "  'answer': 0},\n",
              " {'question': \"What is the number of rows where the value in the column 'Age' is greater than or equal to 0.75 and the value in the column 'Embarked' is 'C'?\",\n",
              "  'answer': 129},\n",
              " {'question': \"What is the number of rows where the value in the column 'Fare' is less than or equal to 12.875 and the value in the column 'Embarked' is 'Q'?\",\n",
              "  'answer': 19}]"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import random, operator\n",
        "from tqdm import tqdm\n",
        "\n",
        "categorical_columns = [\"Sex\", \"Pclass\", \"Embarked\", \"Survived\"]\n",
        "numerical_columns = [\"Age\", \"Fare\", \"SibSp\", \"Parch\"]\n",
        "numerical_ops = [\"<\", \">\", \"==\", \"!=\", \"<=\", \">=\"]\n",
        "operators = {\n",
        "    \"<\": operator.lt,\n",
        "    \">\": operator.gt,\n",
        "    \"==\": operator.eq,\n",
        "    \"!=\": operator.ne,\n",
        "    \"<=\": operator.le,\n",
        "    \">=\": operator.ge,\n",
        "}\n",
        "\n",
        "\n",
        "def generate_random_question(generate_function, df, k, seed=42):\n",
        "    random.seed(seed)\n",
        "    questions = []\n",
        "    for _ in tqdm(range(k)):\n",
        "        question = generate_function(df)\n",
        "        questions.append(question)\n",
        "    return questions\n",
        "\n",
        "\n",
        "def generate_filter_question(df):\n",
        "    # Get a random target column and a random filter column (be careful they should be different)\n",
        "    # Get a random filter value inside the filer column. Avoid NaN values.\n",
        "    filter_col = random.choice(categorical_columns + numerical_columns)\n",
        "    if filter_col in categorical_columns:  # Categorical column\n",
        "        filter_val = random.choice(df[filter_col].dropna().unique())\n",
        "    else:  # Numerical column\n",
        "        filter_val = random.choice(df[filter_col].dropna().unique())\n",
        "\n",
        "    target_col = random.choice(\n",
        "        [col for col in [*categorical_columns, *numerical_columns] if col != filter_col]\n",
        "    )\n",
        "    if target_col in categorical_columns:  # Categorical column\n",
        "        target_val = random.choice(df[target_col].dropna().unique())\n",
        "    else:  # Numerical column\n",
        "        target_val = random.choice(df[target_col].dropna().unique())\n",
        "\n",
        "    filter_op = random.choice(numerical_ops)\n",
        "    target_op = random.choice(numerical_ops)\n",
        "\n",
        "    # Create a question template that take a target column, a filter column and a filter value\n",
        "    # WE ARE ALSO ADDING A TARGET VALUE, OTHERWISE THE ANSWER IS IMPOSSIBLE TO COMPUTE\n",
        "    if filter_col in categorical_columns and target_col in categorical_columns:\n",
        "        template = (\n",
        "            \"Generate a question based on a tabular dataset with this format:\\n\"\n",
        "            \"{df_info}\\n\"\n",
        "            \"The question should be about the number of rows where the value in the column '{target_col}' is '{target_val}' \"\n",
        "            \"and the value in the column '{filter_col}' is '{filter_val}'.\"\n",
        "        )\n",
        "    elif filter_col in numerical_columns and target_col in categorical_columns:\n",
        "        template = (\n",
        "            \"Generate a question based on a tabular dataset with this format:\\n\"\n",
        "            \"{df_info}\\n\"\n",
        "            \"The question should be about the number of rows where the value in the column '{target_col}' is '{target_val}' \"\n",
        "            \"and the value in the column '{filter_col}' is {filter_op} '{filter_val}'.\"\n",
        "        )\n",
        "    elif filter_col in categorical_columns and target_col in numerical_columns:\n",
        "        template = (\n",
        "            \"Generate a question based on a tabular dataset with this format:\\n\"\n",
        "            \"{df_info}\\n\"\n",
        "            \"The question should be about the number of rows where the value in the column '{target_col}' is {target_op} '{target_val}' \"\n",
        "            \"and the value in the column '{filter_col}' is '{filter_val}'.\"\n",
        "        )\n",
        "    else:\n",
        "        template = (\n",
        "            \"Generate a question based on a tabular dataset with this format:\\n\"\n",
        "            \"{df_info}\\n\"\n",
        "            \"The question should be about the number of rows where the value in the column '{target_col}' is {target_op} '{target_val}' \"\n",
        "            \"and the value in the column '{filter_col}' is {filter_op} '{filter_val}'.\"\n",
        "        )\n",
        "\n",
        "    content = template.format(\n",
        "        df_info=df.head(5).to_string(index=False),  # ADD ALL INFO ABOUT DF\n",
        "        target_col=target_col,\n",
        "        target_val=target_val,\n",
        "        filter_col=filter_col,\n",
        "        filter_val=filter_val,\n",
        "        filter_op=filter_op,\n",
        "        target_op=target_op,\n",
        "    )\n",
        "\n",
        "    chat = [\n",
        "        {\"role\": \"user\", \"content\": content},\n",
        "    ]\n",
        "    chat_encoded = tokenizer.apply_chat_template(chat, return_tensors=\"pt\").to(\n",
        "        llm.device\n",
        "    )\n",
        "\n",
        "    llm_outputs_ids = llm.generate(\n",
        "        input_ids=chat_encoded,\n",
        "        generation_config=generation_config,\n",
        "    )[0]\n",
        "    question = tokenizer.decode(\n",
        "        llm_outputs_ids[chat_encoded.size(1) :], skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    if filter_col in categorical_columns and target_col in categorical_columns:\n",
        "        correct_answer = df[\n",
        "            (df[target_col] == target_val) & (df[filter_col] == filter_val)\n",
        "        ].shape[0]\n",
        "    elif filter_col in numerical_columns and target_col in categorical_columns:\n",
        "        correct_answer = df[\n",
        "            (df[target_col] == target_val)\n",
        "            & operators[filter_op](df[filter_col], filter_val)\n",
        "        ].shape[0]\n",
        "    elif filter_col in categorical_columns and target_col in numerical_columns:\n",
        "        correct_answer = df[\n",
        "            operators[target_op](df[target_col], target_val)\n",
        "            & (df[filter_col] == filter_val)\n",
        "        ].shape[0]\n",
        "    else:\n",
        "        correct_answer = df[\n",
        "            operators[target_op](df[target_col], target_val)\n",
        "            & operators[filter_op](df[filter_col], filter_val)\n",
        "        ].shape[0]\n",
        "\n",
        "    # return formatted question and associated answer in a dict {\"question\":[question], \"answer\":[answer]}\n",
        "    return {\"question\": question, \"answer\": correct_answer}\n",
        "\n",
        "\n",
        "generate_random_question(generate_filter_question, df, k=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "il2vCMHRJ74t"
      },
      "outputs": [],
      "source": [
        "# import random\n",
        "# import operator\n",
        "\n",
        "\n",
        "# def generate_random_question(generate_function, df, k=1, seed=42):\n",
        "#     #random.seed(seed)\n",
        "#     return [generate_function(df) for _ in range(k)]\n",
        "\n",
        "\n",
        "# def generate_filter_question(df):\n",
        "#     categorical_columns = [\"Sex\", \"Pclass\", \"Embarked\", \"Survived\"]\n",
        "#     numerical_columns = [\"Age\", \"Fare\", \"SibSp\", \"Parch\"]\n",
        "#     numerical_ops = [\"<\", \">\", \"==\", \"!=\", \"<=\", \">=\"]\n",
        "#     operators = {\n",
        "#         \"<\": operator.lt,\n",
        "#         \">\": operator.gt,\n",
        "#         \"==\": operator.eq,\n",
        "#         \"!=\": operator.ne,\n",
        "#         \"<=\": operator.le,\n",
        "#         \">=\": operator.ge,\n",
        "#     }\n",
        "\n",
        "#     # Get a random target column and a random filter column (be careful they should be different)\n",
        "#     # Get a random filter value inside the filter column. Avoid NaN values.\n",
        "#     filter_column = random.choice(categorical_columns + numerical_columns)\n",
        "\n",
        "#     if filter_column in categorical_columns:  # Categorical column\n",
        "#         filter_value = random.choice(df[filter_column].dropna().unique())\n",
        "#     else:  # Numerical column\n",
        "#         filter_ops = random.choice(numerical_ops)\n",
        "#         filter_value = random.choice(df[filter_column].dropna().unique())\n",
        "\n",
        "#     # Generate a random target column\n",
        "#     target_column = random.choice(\n",
        "#         [\n",
        "#             col\n",
        "#             for col in [*categorical_columns, *numerical_columns]\n",
        "#             if col != filter_column\n",
        "#         ]\n",
        "#     )\n",
        "\n",
        "#     if target_column in categorical_columns:\n",
        "#         target_value = random.choice(df[target_column].dropna().unique())\n",
        "\n",
        "#         if target_column == \"Survived\":\n",
        "#             target_value = \"survived\" if target_value == 1 else \"not survived\"\n",
        "#         elif target_column == \"Pclass\":\n",
        "#             target_value = f\"are in class {target_value}\"\n",
        "#         elif target_column == \"Embarked\":\n",
        "#             target_value = f\"embarked in {target_value}\"\n",
        "#         elif target_column == \"Sex\":\n",
        "#             target_value = f\"are {target_value}\"\n",
        "#     else:\n",
        "#         target_ops = random.choice(numerical_ops)\n",
        "#         target_value = random.choice(df[target_column].dropna().unique())\n",
        "\n",
        "#     # Create a question template that take a target column, a filter column and a filter value\n",
        "#     # Compute the correct answer for the given target column, filter column and filter value\n",
        "#     if filter_column in categorical_columns and target_column in categorical_columns:\n",
        "#         question = (\n",
        "#             f\"How many passengers {target_value} and {filter_column} is {filter_value}?\"\n",
        "#         )\n",
        "#         correct_answer = df[\n",
        "#             (df[filter_column] == filter_value) & (df[target_column] == target_value)\n",
        "#         ][target_column].count()\n",
        "#     elif filter_column in numerical_columns and target_column in categorical_columns:\n",
        "#         question = f\"How many passengers {target_value} and {filter_column} {filter_ops} {filter_value}?\"\n",
        "#         correct_answer = df[\n",
        "#             operators[filter_ops](df[filter_column], filter_value)\n",
        "#             & (df[target_column] == target_value)\n",
        "#         ][target_column].count()\n",
        "#     elif filter_column in categorical_columns and target_column in numerical_columns:\n",
        "#         question = f\"How many passengers {target_column} {target_ops} {target_value} and {filter_column} is {filter_value}?\"\n",
        "#         correct_answer = df[\n",
        "#             operators[target_ops](df[target_column], target_value)\n",
        "#             & (df[filter_column] == filter_value)\n",
        "#         ][target_column].count()\n",
        "#     else:\n",
        "#         question = f\"How many passengers {target_column} {target_ops} {target_value} and {filter_column} {filter_ops} {filter_value}?\"\n",
        "#         correct_answer = df[\n",
        "#             operators[target_ops](df[target_column], target_value)\n",
        "#             & operators[filter_ops](df[filter_column], filter_value)\n",
        "#         ][target_column].count()\n",
        "\n",
        "#     # return formatted question and associated answer in a dict {\"question\":[question], \"answer\":[answer]}\n",
        "#     return {\"question\": question, \"answer\": correct_answer}\n",
        "\n",
        "\n",
        "# generate_random_question(generate_filter_question, df, k=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzDJ9c3VhHt2"
      },
      "source": [
        "## 1.4 Guided Introduction: The Evaluation.\n",
        "\n",
        "The last step in this section is to evaluate our model on 20 random questions! We'll use simple accuracy.\n",
        "\n",
        "You should have an accuracy between 0.9 and 1.\n",
        "\n",
        "<font color='red'>TODO: Follow instruction in comment of the cell below.</font>\n",
        "\n",
        "<font color='green'>BONUS: Investigate on errors and improve our prompt/parsing to solve them.</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "xiE53oszRGcq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating random questions\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [01:13<00:00,  3.66s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting the evaluation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [01:42<00:00,  5.14s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 20/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Generate 20 random question\n",
        "print(\"Generating random questions\")\n",
        "questions = generate_random_question(generate_filter_question, df, k=20)\n",
        "sum_correct = 0\n",
        "incorrect_answers = []\n",
        "\n",
        "# Iterate over question to format prompt, generate answer and execute answer.\n",
        "print(\"\\nStarting the evaluation\")\n",
        "for q in tqdm(questions):\n",
        "    prompt = example_prompt_template.format(\n",
        "        var_name=\"df\",\n",
        "        df_info=df.head(5).to_string(index=False),\n",
        "        user_question=q[\"question\"],\n",
        "    )\n",
        "    answer = generate_answer(prompt)\n",
        "    res = exec_answer(answer, q[\"answer\"])\n",
        "    if res == False:\n",
        "        incorrect_answers.append((q, answer))\n",
        "\n",
        "    sum_correct += res\n",
        "\n",
        "# Report the Accuracy\n",
        "print(f\"Accuracy: {sum_correct}/{len(questions)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Incorrect answers: []\n"
          ]
        }
      ],
      "source": [
        "print(\"Incorrect answers:\", incorrect_answers)\n",
        "\n",
        "# WRITE THAT OUR ERRORS ARE DUE TO THE FACT THAT THE OUTPUT IS A DATAFRAME"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LGYL5LUeiEBI"
      },
      "source": [
        "## 2. More Questions.\n",
        "\n",
        "Now it's your turn to imagine a type of question (\"How many ...\"). Implement a function to generate new type of question. Verify that our previous code work with your new question then evaluate it.\n",
        "\n",
        "<font color='red'>TODO: Generate **AT LEAST ONE** new type of question and report this new question accuracy.</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def generate_fare_question(df):\n",
        "    # Get a random target column and a random filter column (be careful they should be different)\n",
        "    # Get a random filter value inside the filer column. Avoid NaN values.\n",
        "    filter_col = random.choice(\n",
        "        [col for col in [*categorical_columns, *numerical_columns] if col != \"Fare\"]\n",
        "    )\n",
        "    filter_val = random.choice(df[filter_col].dropna().unique())\n",
        "    filter_op = random.choice(numerical_ops)\n",
        "\n",
        "    # Create a question template that take a target column, a filter column and a filter value\n",
        "    if filter_col in categorical_columns:\n",
        "        template = (\n",
        "            \"Generate a question based on a tabular dataset with this format:\\n\"\n",
        "            \"{df_info}\\n\"\n",
        "            \"The question should be about the total fare for the passengers where \"\n",
        "            \"the value in the column '{filter_col}' is '{filter_val}'.\"\n",
        "            \"Here are some examples of questions:\"\n",
        "            \"1) What is the total fare for the passengers who survived?\"\n",
        "            \"2) How much did the passengers who embarked in S pay in total?\"\n",
        "            \"3) What is the total fare for male passengers?\"\n",
        "        )\n",
        "    else:\n",
        "        template = (\n",
        "            \"Generate a question based on a tabular dataset with this format:\\n\"\n",
        "            \"{df_info}\\n\"\n",
        "            \"The question should be about the total fare for the passengers where \"\n",
        "            \"the value in the column '{filter_col}' is {filter_op} '{filter_val}'.\"\n",
        "            \"Here are some examples of questions:\"\n",
        "            \"1) What is the total fare for the passengers who are older than 30?\"\n",
        "            \"2) How much did the passengers who paid more than 50 in total?\"\n",
        "            \"3) What is the total fare for passengers who have more than 2 siblings?\"\n",
        "        )\n",
        "\n",
        "    content = template.format(\n",
        "        df_info=df.head(5).to_string(index=False),\n",
        "        filter_col=filter_col,\n",
        "        filter_val=filter_val,\n",
        "        filter_op=filter_op,\n",
        "    )\n",
        "\n",
        "    chat = [\n",
        "        {\"role\": \"user\", \"content\": content},\n",
        "    ]\n",
        "    chat_encoded = tokenizer.apply_chat_template(chat, return_tensors=\"pt\").to(\n",
        "        llm.device\n",
        "    )\n",
        "\n",
        "    llm_outputs_ids = llm.generate(\n",
        "        input_ids=chat_encoded,\n",
        "        generation_config=generation_config,\n",
        "    )[0]\n",
        "    question = tokenizer.decode(\n",
        "        llm_outputs_ids[chat_encoded.size(1) :], skip_special_tokens=True\n",
        "    )\n",
        "\n",
        "    # Compute the correct answer for the given target column, filter column and filter value.\n",
        "    if filter_col in categorical_columns:\n",
        "        correct_answer = df[(df[filter_col] == filter_val)][\"Fare\"].sum()\n",
        "    else:\n",
        "        correct_answer = df[operators[filter_op](df[filter_col], filter_val)][\n",
        "            \"Fare\"\n",
        "        ].sum()\n",
        "\n",
        "    # return formatted question and associated answer in a dict {\"question\":[question], \"answer\":[answer]}\n",
        "    return {\"question\": question, \"answer\": correct_answer}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating random questions\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [00:47<00:00,  2.38s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting the evaluation\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20/20 [01:26<00:00,  4.32s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 20/20\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Generate 20 random question\n",
        "print(\"Generating random questions\")\n",
        "questions = generate_random_question(generate_fare_question, df, k=20)\n",
        "sum_correct = 0\n",
        "incorrect_answers = []\n",
        "\n",
        "# Iterate over question to format prompt, generate answer and execute answer.\n",
        "print(\"\\nStarting the evaluation\")\n",
        "for q in tqdm(questions):\n",
        "    prompt = example_prompt_template.format(\n",
        "        var_name=\"df\",\n",
        "        df_info=df.head(5).to_string(index=False),\n",
        "        user_question=q[\"question\"],\n",
        "    )\n",
        "    answer = generate_answer(prompt)\n",
        "    res = exec_answer(answer, q[\"answer\"])\n",
        "    if res == False:\n",
        "        incorrect_answers.append((prompt, q[\"answer\"], answer))\n",
        "\n",
        "    sum_correct += res\n",
        "\n",
        "# Report the Accuracy\n",
        "print(f\"Accuracy: {sum_correct}/{len(questions)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "for ia in incorrect_answers:\n",
        "    print(ia[0])\n",
        "    print(\"\\n*****\\n\")\n",
        "    print(ia[1])\n",
        "    print(\"\\n*****\\n\")\n",
        "    print(ia[2])\n",
        "    print(\"\\n*****\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mt35rcuMoAdW"
      },
      "source": [
        "## 3. More datasets.\n",
        "\n",
        "Below we load a new dataset: \"adult_income_dataset\".\n",
        "\n",
        "<font color='red'>TODO: Evaluate our questions on this new dataset. Report the accuracy. Comment Any differences.</font>\n",
        "\n",
        "<font color='green'>BONUS: Try to find a prompt that answer this question: What is the mean salary of titanic surviror based on adult dataset.</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "z33vyj_Sk_WX"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 48842 entries, 0 to 48841\n",
            "Data columns (total 15 columns):\n",
            " #   Column           Non-Null Count  Dtype \n",
            "---  ------           --------------  ----- \n",
            " 0   age              48842 non-null  int64 \n",
            " 1   workclass        48842 non-null  object\n",
            " 2   fnlwgt           48842 non-null  int64 \n",
            " 3   education        48842 non-null  object\n",
            " 4   educational-num  48842 non-null  int64 \n",
            " 5   marital-status   48842 non-null  object\n",
            " 6   occupation       48842 non-null  object\n",
            " 7   relationship     48842 non-null  object\n",
            " 8   race             48842 non-null  object\n",
            " 9   gender           48842 non-null  object\n",
            " 10  capital-gain     48842 non-null  int64 \n",
            " 11  capital-loss     48842 non-null  int64 \n",
            " 12  hours-per-week   48842 non-null  int64 \n",
            " 13  native-country   48842 non-null  object\n",
            " 14  income           48842 non-null  object\n",
            "dtypes: int64(6), object(9)\n",
            "memory usage: 5.6+ MB\n"
          ]
        }
      ],
      "source": [
        "adult = pd.read_csv(\"hf://datasets/meghana/adult_income_dataset/adult.csv\")\n",
        "adult.info()\n",
        "\n",
        "titanic = df"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
